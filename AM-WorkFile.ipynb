{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport json\\nimport requests\\nfrom json2html import *\\n\\n# string interpolation\\npage_number = 1\\nurl = 'https://www.usnews.com/best-colleges/api/search?_sort=rank&_sortDirection=asc&schoolType=national-universities&ranking=social-mobility&_page={}'.format(page_number)\\n\\nreq = requests.get(url, headers={'User-Agent':'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-GB; rv:1.8.1.6) Gecko/20070725'})\\n\\na = req.json() # dictionary\\n#print(a['institution']) # looking to see the schools. type find institution to see\\n\\n#print(a['meta_tags']) #beginning part\\ndata = json.dumps(a) # string or json\\nprint(json.dumps(a, indent = 4, sort_keys = True)) # prettifies json web page\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import json\n",
    "import requests\n",
    "from json2html import *\n",
    "\n",
    "# string interpolation\n",
    "page_number = 1\n",
    "url = 'https://www.usnews.com/best-colleges/api/search?_sort=rank&_sortDirection=asc&schoolType=national-universities&ranking=social-mobility&_page={}'.format(page_number)\n",
    "\n",
    "req = requests.get(url, headers={'User-Agent':'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-GB; rv:1.8.1.6) Gecko/20070725'})\n",
    "\n",
    "a = req.json() # dictionary\n",
    "#print(a['institution']) # looking to see the schools. type find institution to see\n",
    "\n",
    "#print(a['meta_tags']) #beginning part\n",
    "data = json.dumps(a) # string or json\n",
    "print(json.dumps(a, indent = 4, sort_keys = True)) # prettifies json web page\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 71)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m71\u001b[0m\n\u001b[1;33m    except Exception as e:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# Step 1: pull data for page #1 [DONE]\n",
    "# Step 2: get data for first school in current page [DONE]\n",
    "# Step 3: organize data from Step 2 into a dictionary [DONE]\n",
    "# Step 4: append dictionary into the list buffer (which in your case is “education”) so that we can give pandas a list of dictionaries to make into a data frame. [DONE]\n",
    "# Step 5: Repeat Steps 2-4 until we run out of schools on the current page. Question to ask here: how do we know when we’ve run out of schools in the current page? Hint: try/except! [Done]\n",
    "# Step 6: Increment the current page to the next page and repeat Steps 1-5 until we run out of pages. How do we know when we’ve run out of pages?? [Done]\n",
    "# Step 7: by now you should have your list buffer (“education”) filled with a dictionaries container data on all the schools in the website. Put it into a data frame! [Done]\n",
    "# Step 8: Save the data frame\n",
    "# Step 9: Iterate thru steps 1-8 for all categories\n",
    "\n",
    "# imports\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "    \n",
    "# Step 9: Iterate through steps 1-8 for all categories\n",
    "categories = [\n",
    "    'veterans',\n",
    "    'top-public',\n",
    "    'undergraduate-teaching',\n",
    "    'social-mobility',\n",
    "    'innovative',\n",
    "    'best-value'\n",
    "]\n",
    "for category in categories:\n",
    "    education = [] # list buffer\n",
    "\n",
    "    # Step 1: pull data for page #1\n",
    "    page_number = 1\n",
    "\n",
    "    # Step 6: Increment the current page to the next page and repeat Steps 1-5 until we run out of pages. How do we know when we’ve run out of pages??\n",
    "    while True:\n",
    "        try:\n",
    "            url = 'https://www.usnews.com/best-colleges/api/search?\n",
    "                _sort=rank&_sortDirection=asc&schoolType=national-universities&ranking={category}&_page={page}'\n",
    "                .format(category=category, page=page_number)\n",
    "            req = requests.get(\n",
    "                url, \n",
    "                headers={\n",
    "                    'User-Agent':'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-GB; rv:1.8.1.6) Gecko/20070725'\n",
    "                }\n",
    "            )\n",
    "            a = req.json() # dictionary\n",
    "\n",
    "            # Step 2: get data for first school in current page\n",
    "            i = 0\n",
    "\n",
    "            # Step 5: Repeat Steps 2-4 until we run out of schools on the current page. Question to ask here: how do we know when we’ve run out of schools in the current page? Hint: try/except!\n",
    "            while True:\n",
    "                try:\n",
    "                    # Step 3: organize data from Step 2 into a dictionary\n",
    "                    institution = a['data']['items'][i]['institution']\n",
    "                    searchData = a['data']['items'][i]['searchData']\n",
    "                    costAfterAid = searchData['costAfterAid']['rawValue']\n",
    "                    enrollment = searchData['enrollment']['rawValue']\n",
    "                    d = {\n",
    "                        'schools'      : institution['displayName'],\n",
    "                        'ranks'        : institution['rankingSortRank'],\n",
    "                        'city'         : institution['city'],\n",
    "                        'state'        : institution['state'],\n",
    "                        'costAfterAid' : float(costAfterAid) if costAfterAid else None,\n",
    "                        'enrollment'   : int(enrollment) if enrollment else None\n",
    "                    }\n",
    "\n",
    "                    # Step 4: append dictionary into the list buffer (which in your case is “education”) so that we can give pandas a list of dictionaries to make into a data frame.\n",
    "                    education.append(d)\n",
    "                    i = i + 1 # incrementers\n",
    "                except Exception as e:\n",
    "                    break\n",
    "            page_number = page_number + 1 # incrementers \n",
    "        except Exception as e:\n",
    "            break   \n",
    "\n",
    "    # Step 7: by now you should have your list buffer (“education”) filled with a dictionaries container data on all the schools in the website. Put it into a data frame!\n",
    "    df = pd.DataFrame(education)\n",
    "    df.head()\n",
    "\n",
    "    # Step 8: Save the data frame\n",
    "    #df.to_csv('output/{}.csv'.format(category), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
